{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Build and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm          # Importing statsmodels for linear model \n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score as rsq\n",
    "#Random Forest Regrssor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# for integer encoding using feature-engine\n",
    "from feature_engine.categorical_encoders import OrdinalCategoricalEncoder\n",
    "#for XGBOOST \n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "data=pd.read_csv(r\"data_no_null.csv\",encoding='latin1')# reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns='Unnamed: 0',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing and data cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### droping some unnecessary columns \n",
    "\n",
    "deletion1:- here i m dropping subgrade column because its cardinality is high as well as if i will group them then it will give similar column which i already have as Grade \n",
    "\n",
    "deletion2:- dropping state columns because it have ['FL', 'MD', 'OH', ..., 'NH', 'MD', 'FL'] these types of values which also not giving any infrmation about the states so irrelevent for user input \n",
    "\n",
    "deletion3:-i m deleting this EMP_designation column because it is having high cardinality and not much information to group it , and make it low cardinality \n",
    "\n",
    "deletion4 :- i m deleting this last_week_pay column because it is having high cardinality and not much information to group it , and make it low cardinality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns not necessary for prediction\n",
    "cols_to_drop=['sub_grade','State','Emp_designation','last_week_pay']\n",
    "# dropping the unnecessary columns\n",
    "data_1=data.drop(columns=cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['terms',\n",
       " 'grade',\n",
       " 'home_ownership',\n",
       " 'verification_status',\n",
       " 'purpose',\n",
       " 'initial_list_status',\n",
       " 'application_type',\n",
       " 'Experience']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical = [col for col in data_1.columns if data_1[col].dtypes == 'O']\n",
    "\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here , i m making 'purpose' column as low cardinality by grouping labels to similar labels\n",
    "exaple:-\n",
    "\n",
    "        lable1:-debt_consolidation                as debt_consolidation\n",
    "\n",
    "        label2:-credit_card                       as credit_card\n",
    "        \n",
    "        label3:-home_improvement                  as home_improvement\n",
    "        \n",
    "        lable4:-major_purchase+car+house          as major_purchase\n",
    "        \n",
    "        lable5:-small_business+renewable_energy   as small_business\n",
    "        \n",
    "        label6:-weddings+others+vacations+moving  as others\n",
    "        \n",
    "        label7:-medical+educations                as medical(or)education\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "educational              423\n",
       "renewable_energy         575\n",
       "wedding                 2347\n",
       "house                   3707\n",
       "vacation                4736\n",
       "moving                  5414\n",
       "medical                 8540\n",
       "car                     8863\n",
       "small_business         10377\n",
       "major_purchase         17277\n",
       "other                  42894\n",
       "home_improvement       51829\n",
       "credit_card           206182\n",
       "debt_consolidation    524215\n",
       "Name: purpose, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.purpose.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[['A','B']] = df[['A','B']].replace([1, 3, 2], [3, 6, 7])\n",
    "data_1[['purpose']] = data_1[['purpose']].replace(['car','house','renewable_energy','wedding','vacation','moving','medical','educational'],\n",
    "                                              ['major_purchase','major_purchase','small_business','other','other','other','medical(or)education','medical(or)education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medical(or)education      8963\n",
       "small_business           10952\n",
       "major_purchase           29847\n",
       "home_improvement         51829\n",
       "other                    55391\n",
       "credit_card             206182\n",
       "debt_consolidation      524215\n",
       "Name: purpose, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.purpose.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JOINT            511\n",
       "INDIVIDUAL    886868\n",
       "Name: application_type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.application_type.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loan_amnt ', 'terms', 'Rate_of_intrst', 'grade', 'home_ownership',\n",
       "       'annual_inc', 'verification_status', 'purpose', 'debt_income_ratio',\n",
       "       'delinq_2yrs', 'inq_last_6mths', 'numb_credit', 'pub_rec',\n",
       "       'total revol_bal', 'total_credits', 'initial_list_status',\n",
       "       'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
       "       'collection_recovery_fee', 'collections_12_mths_ex_med',\n",
       "       'application_type', 'acc_now_delinq', 'Experience',\n",
       "       'mths_since_last_delinq', 'tot_curr_bal', 'tot_colle_amt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to split the datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for one hot encoding with feature-engine\n",
    "from feature_engine.categorical_encoders import OneHotCategoricalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((621165, 26), (266214, 26))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's separate into training and testing set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_1[['loan_amnt ', 'terms', 'Rate_of_intrst', 'grade', 'home_ownership','annual_inc', 'verification_status',\n",
    "            'purpose', 'debt_income_ratio','delinq_2yrs', 'inq_last_6mths', 'numb_credit', 'pub_rec','total_credits', \n",
    "            'initial_list_status','total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', \n",
    "            'collections_12_mths_ex_med', 'application_type', 'acc_now_delinq', 'Experience', 'mths_since_last_delinq',\n",
    "            'tot_curr_bal', 'tot_colle_amt']],  # predictors\n",
    "    data_1[['total revol_bal']],  # target\n",
    "    test_size=0.3,  # percentage of obs in test set\n",
    "    random_state=0)  # seed to ensure reproducibility\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['terms',\n",
       " 'grade',\n",
       " 'home_ownership',\n",
       " 'verification_status',\n",
       " 'purpose',\n",
       " 'initial_list_status',\n",
       " 'application_type',\n",
       " 'Experience']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n",
    "\n",
    "categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "    -  It simply creates additional features based on the number of unique values in the categorical feature. Every unique            value in the category will be added as a feature. One-Hot Encoding is the process of creating dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotCategoricalEncoder(variables=['terms', 'grade', 'home_ownership',\n",
       "                                    'verification_status', 'purpose',\n",
       "                                    'initial_list_status', 'application_type',\n",
       "                                    'Experience'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_enc = OneHotCategoricalEncoder(\n",
    "    variables=categorical,\n",
    "    drop_last=False)\n",
    "\n",
    "ohe_enc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'terms': array(['60 months', '36 months'], dtype=object),\n",
       " 'grade': array(['D', 'C', 'A', 'B', 'F', 'E', 'G'], dtype=object),\n",
       " 'home_ownership': array(['MORTGAGE', 'RENT', 'OWN', 'OTHER', 'NONE', 'ANY'], dtype=object),\n",
       " 'verification_status': array(['Verified', 'Source Verified', 'Not Verified'], dtype=object),\n",
       " 'purpose': array(['debt_consolidation', 'credit_card', 'home_improvement',\n",
       "        'small_business', 'other', 'major_purchase',\n",
       "        'medical(or)education'], dtype=object),\n",
       " 'initial_list_status': array(['w', 'f'], dtype=object),\n",
       " 'application_type': array(['INDIVIDUAL', 'JOINT'], dtype=object),\n",
       " 'Experience': array(['10+ years', '2 years', '4 years', '< 1 year', '3 years',\n",
       "        '7 years', '8 years', '1 year', '6 years', '5 years', '9 years'],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the encoder dict we can observe each of the top categories\n",
    "# selected for each of the variables\n",
    "\n",
    "ohe_enc.encoder_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>Rate_of_intrst</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>debt_income_ratio</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>numb_credit</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>total_credits</th>\n",
       "      <th>total_rec_int</th>\n",
       "      <th>...</th>\n",
       "      <th>Experience_2 years</th>\n",
       "      <th>Experience_4 years</th>\n",
       "      <th>Experience_&lt; 1 year</th>\n",
       "      <th>Experience_3 years</th>\n",
       "      <th>Experience_7 years</th>\n",
       "      <th>Experience_8 years</th>\n",
       "      <th>Experience_1 year</th>\n",
       "      <th>Experience_6 years</th>\n",
       "      <th>Experience_5 years</th>\n",
       "      <th>Experience_9 years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>416835</th>\n",
       "      <td>35000</td>\n",
       "      <td>16.55</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>25.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>911.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596073</th>\n",
       "      <td>10000</td>\n",
       "      <td>12.29</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>13.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>901.03</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759330</th>\n",
       "      <td>8650</td>\n",
       "      <td>8.94</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>710.73</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398838</th>\n",
       "      <td>5000</td>\n",
       "      <td>11.53</td>\n",
       "      <td>66000.0</td>\n",
       "      <td>34.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>384.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401686</th>\n",
       "      <td>12000</td>\n",
       "      <td>9.99</td>\n",
       "      <td>84000.0</td>\n",
       "      <td>10.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>932.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        loan_amnt   Rate_of_intrst  annual_inc  debt_income_ratio  \\\n",
       "416835       35000           16.55    125000.0              25.83   \n",
       "596073       10000           12.29     24000.0              13.70   \n",
       "759330        8650            8.94     30000.0              11.80   \n",
       "398838        5000           11.53     66000.0              34.45   \n",
       "401686       12000            9.99     84000.0              10.31   \n",
       "\n",
       "        delinq_2yrs  inq_last_6mths  numb_credit  pub_rec  total_credits  \\\n",
       "416835          0.0             3.0         19.0      0.0           52.0   \n",
       "596073          0.0             1.0          6.0      0.0           27.0   \n",
       "759330          0.0             0.0          5.0      0.0           21.0   \n",
       "398838          0.0             1.0         15.0      0.0           22.0   \n",
       "401686          0.0             0.0          8.0      4.0           12.0   \n",
       "\n",
       "        total_rec_int  ...  Experience_2 years  Experience_4 years  \\\n",
       "416835         911.92  ...                   0                   0   \n",
       "596073         901.03  ...                   1                   0   \n",
       "759330         710.73  ...                   0                   1   \n",
       "398838         384.60  ...                   0                   0   \n",
       "401686         932.96  ...                   0                   0   \n",
       "\n",
       "        Experience_< 1 year  Experience_3 years  Experience_7 years  \\\n",
       "416835                    0                   0                   0   \n",
       "596073                    0                   0                   0   \n",
       "759330                    0                   0                   0   \n",
       "398838                    0                   0                   0   \n",
       "401686                    1                   0                   0   \n",
       "\n",
       "        Experience_8 years  Experience_1 year  Experience_6 years  \\\n",
       "416835                   0                  0                   0   \n",
       "596073                   0                  0                   0   \n",
       "759330                   0                  0                   0   \n",
       "398838                   0                  0                   0   \n",
       "401686                   0                  0                   0   \n",
       "\n",
       "        Experience_5 years  Experience_9 years  \n",
       "416835                   0                   0  \n",
       "596073                   0                   0  \n",
       "759330                   0                   0  \n",
       "398838                   0                   0  \n",
       "401686                   0                   0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = ohe_enc.transform(X_train)\n",
    "X_test = ohe_enc.transform(X_test)\n",
    "\n",
    "# let's explore the result\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>Rate_of_intrst</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>debt_income_ratio</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>numb_credit</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>total_credits</th>\n",
       "      <th>total_rec_int</th>\n",
       "      <th>...</th>\n",
       "      <th>Experience_2 years</th>\n",
       "      <th>Experience_4 years</th>\n",
       "      <th>Experience_&lt; 1 year</th>\n",
       "      <th>Experience_3 years</th>\n",
       "      <th>Experience_7 years</th>\n",
       "      <th>Experience_8 years</th>\n",
       "      <th>Experience_1 year</th>\n",
       "      <th>Experience_6 years</th>\n",
       "      <th>Experience_5 years</th>\n",
       "      <th>Experience_9 years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>372989</th>\n",
       "      <td>10000</td>\n",
       "      <td>13.99</td>\n",
       "      <td>47000.0</td>\n",
       "      <td>12.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>754.43</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9769</th>\n",
       "      <td>18000</td>\n",
       "      <td>12.99</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>19.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6287.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134461</th>\n",
       "      <td>6000</td>\n",
       "      <td>15.10</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>28.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1333.97</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425772</th>\n",
       "      <td>12950</td>\n",
       "      <td>14.99</td>\n",
       "      <td>36899.0</td>\n",
       "      <td>15.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2125.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409080</th>\n",
       "      <td>1900</td>\n",
       "      <td>9.99</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>23.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>148.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        loan_amnt   Rate_of_intrst  annual_inc  debt_income_ratio  \\\n",
       "372989       10000           13.99     47000.0              12.69   \n",
       "9769         18000           12.99     60000.0              19.66   \n",
       "134461        6000           15.10     50000.0              28.01   \n",
       "425772       12950           14.99     36899.0              15.88   \n",
       "409080        1900            9.99     42000.0              23.06   \n",
       "\n",
       "        delinq_2yrs  inq_last_6mths  numb_credit  pub_rec  total_credits  \\\n",
       "372989          0.0             0.0         14.0      0.0           15.0   \n",
       "9769            0.0             0.0          9.0      0.0           23.0   \n",
       "134461          1.0             1.0         11.0      0.0           19.0   \n",
       "425772          0.0             2.0          7.0      0.0           21.0   \n",
       "409080          0.0             1.0         11.0      0.0           22.0   \n",
       "\n",
       "        total_rec_int  ...  Experience_2 years  Experience_4 years  \\\n",
       "372989         754.43  ...                   0                   0   \n",
       "9769          6287.44  ...                   0                   0   \n",
       "134461        1333.97  ...                   0                   0   \n",
       "425772        2125.63  ...                   0                   0   \n",
       "409080         148.69  ...                   0                   0   \n",
       "\n",
       "        Experience_< 1 year  Experience_3 years  Experience_7 years  \\\n",
       "372989                    0                   0                   0   \n",
       "9769                      0                   1                   0   \n",
       "134461                    0                   0                   0   \n",
       "425772                    1                   0                   0   \n",
       "409080                    1                   0                   0   \n",
       "\n",
       "        Experience_8 years  Experience_1 year  Experience_6 years  \\\n",
       "372989                   0                  0                   0   \n",
       "9769                     0                  0                   0   \n",
       "134461                   0                  0                   0   \n",
       "425772                   0                  0                   0   \n",
       "409080                   0                  0                   0   \n",
       "\n",
       "        Experience_5 years  Experience_9 years  \n",
       "372989                   0                   0  \n",
       "9769                     0                   0  \n",
       "134461                   0                   0  \n",
       "425772                   0                   0  \n",
       "409080                   0                   0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=data_1.iloc[[0]]\n",
    "d_1=d[['loan_amnt ', 'terms', 'Rate_of_intrst', 'grade', 'home_ownership','annual_inc', 'verification_status',\n",
    "            'purpose', 'debt_income_ratio','delinq_2yrs', 'inq_last_6mths', 'numb_credit', 'pub_rec','total_credits', \n",
    "            'initial_list_status','total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', \n",
    "            'collections_12_mths_ex_med', 'application_type', 'acc_now_delinq', 'Experience', 'mths_since_last_delinq',\n",
    "            'tot_curr_bal', 'tot_colle_amt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#saving the model to the local file system\n",
    "filename = 'finalized_model.pickle'\n",
    "pickle.dump(xgb_clf, open(filename, 'wb'))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filehandler = encoder.pickle\n",
    "pickle.dump(ohe_enc,open(filehandler,'wb'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(ohe_enc , 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = joblib.load('model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = ohe_enc.transform(X_test)\n",
    "d_test = encoder.transform(d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''# the scaler - for standardisation\n",
    "from sklearn.preprocessing import StandardScaler'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# standardisation: with the StandardScaler from sklearn\n",
    "\n",
    "# set up the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "#X_test_scaled = scaler.transform(X_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(scaler , 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = joblib.load('scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a normalisation function \n",
    "def normalize (x): \n",
    "    return ( (x-np.min(x))/ (max(x) - min(x)))\n",
    "                                            \n",
    "                                              \n",
    "# applying normalize ( ) to all columns \n",
    "X_train = X_train.apply(normalize) \n",
    "y_train = y_train.apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm          # Importing statsmodels\n",
    "X_train = sm.add_constant(X_train)    # Adding a constant column to our dataframe\n",
    "# create a first fitted model\n",
    "lm_1 = sm.OLS(y_train,X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the summary of our first linear model\n",
    "print(lm_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "df=pd.read_csv(r\"C:\\Users\\sak\\Desktop\\excelr_proj_1\\data_no_null.csv\",encoding='latin1')# reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [col for col in df.columns if df[col].dtypes == 'O']\n",
    "\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=df.drop(columns=categorical,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a normalisation function \n",
    "def normalize (x): \n",
    "    return ( (x-np.min(x))/ (max(x) - min(x)))\n",
    "\n",
    "df_1 = df_1.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate into training and testing set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_1[['loan_amnt ', 'Rate_of_intrst', 'annual_inc', 'debt_income_ratio',\n",
    "       'delinq_2yrs', 'inq_last_6mths', 'numb_credit', 'pub_rec',\n",
    "        'total_credits', 'total_rec_int',\n",
    "       'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
    "       'collections_12_mths_ex_med', 'acc_now_delinq',\n",
    "       'mths_since_last_delinq', 'tot_curr_bal', 'tot_colle_amt']],  # predictors\n",
    "    df_1[['total revol_bal']],  # target\n",
    "    test_size=0.3,  # percentage of obs in test set\n",
    "    random_state=0)  # seed to ensure reproducibility\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)    # Adding a constant column to our dataframe\n",
    "# create a first fitted model\n",
    "lm_1 = sm.OLS(y_train,X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the summary of our first linear model\n",
    "print(lm_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "df2=pd.read_csv(r\"C:\\Users\\sak\\Desktop\\excelr_proj_1\\data (1).csv\",encoding='latin1')# reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_a=df2.drop(columns=['member_id ','batch_ID ','verification_status_joint','mths_since_last_major_derog',\n",
    "                        'mths_since_last_record','mths_since_last_delinq'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [col for col in df2_a.columns if df2_a[col].dtypes == 'O']\n",
    "\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_b=df2_a.drop(columns=categorical,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c = df2_b.dropna(how='any',axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#defining a normalisation function \n",
    "def normalize (x): \n",
    "    return ( (x-np.min(x))/ (max(x) - min(x)))\n",
    "\n",
    "df2_c = df2_c.apply(normalize)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_c.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df2_c[['loan_amnt ', 'Rate_of_intrst', 'annual_inc', 'debt_income_ratio',\n",
    "       'delinq_2yrs', 'inq_last_6mths', 'numb_credit', 'pub_rec',\n",
    "        'total_credits', 'total_rec_int',\n",
    "       'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
    "       'collections_12_mths_ex_med', 'acc_now_delinq', 'tot_colle_amt',\n",
    "       'tot_curr_bal']]\n",
    "Y=df2_c[['total revol_bal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "Y = sc_y.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate into training and testing set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X  ,  # predictors\n",
    "                                                    Y,  # target\n",
    "    test_size=0.3,  # percentage of obs in test set\n",
    "    random_state=0)  # seed to ensure reproducibility\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)    # Adding a constant column to our dataframe\n",
    "# create a first fitted model\n",
    "lm_1 = sm.OLS(y_train,X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the summary of our first linear model\n",
    "print(lm_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=pd.DataFrame(data=sc_y.inverse_transform(y_test),columns=['revolving_balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_m6 = sm.add_constant(X_test)\n",
    "X_test_cons = sm.add_constant(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction= sc_y.inverse_transform(lm_1.predict(X_test_cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.insert(loc=1,column='prediction',value=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "print('RMSE :', np.sqrt(metrics.mean_squared_error(y_test['revolving_balance'], y_test['prediction'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data ,data having no null values \n",
    "df3=pd.read_csv(r\"C:\\Users\\sak\\Desktop\\excelr_proj_1\\data_no_null.csv\",encoding='latin1')# reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns not necessary for prediction\n",
    "cols_to_drop=['Unnamed: 0','sub_grade','State','Emp_designation','last_week_pay']\n",
    "# dropping the unnecessary columns\n",
    "df3_a=df3.drop(columns=cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [col for col in df3_a.columns if df3_a[col].dtypes == 'O']\n",
    "\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[['A','B']] = df[['A','B']].replace([1, 3, 2], [3, 6, 7])\n",
    "df3_a[['purpose']] = df3_a[['purpose']].replace(['car','house','renewable_energy','wedding','vacation','moving','medical','educational'],\n",
    "                                              ['major_purchase','major_purchase','small_business','other','other','other','medical(or)education','medical(or)education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_a.purpose.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.categorical_encoders import CountFrequencyCategoricalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_enc = CountFrequencyCategoricalEncoder(\n",
    "    encoding_method='frequency', # to do frequency ==> encoding_method='frequency'\n",
    "    variables=categorical)\n",
    "\n",
    "df3_encoded = count_enc.fit_transform(df3_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_enc.encoder_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df3_encoded[['loan_amnt ', 'terms', 'Rate_of_intrst', 'grade', 'home_ownership',\n",
    "       'annual_inc', 'verification_status', 'purpose', 'debt_income_ratio',\n",
    "       'delinq_2yrs', 'inq_last_6mths', 'numb_credit', 'pub_rec',\n",
    "        'total_credits', 'initial_list_status',\n",
    "       'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
    "       'collection_recovery_fee', 'collections_12_mths_ex_med',\n",
    "       'application_type', 'acc_now_delinq', 'Experience',\n",
    "       'mths_since_last_delinq', 'tot_curr_bal', 'tot_colle_amt']]\n",
    "Y=df3_encoded[['total revol_bal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "Y = sc_y.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate into training and testing set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X  ,  # predictors\n",
    "                                                    Y,  # target\n",
    "    test_size=0.3,  # percentage of obs in test set\n",
    "    random_state=0)  # seed to ensure reproducibility\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)    # Adding a constant column to our dataframe\n",
    "# create a first fitted model\n",
    "lm_1 = sm.OLS(y_train,X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the summary of our first linear model\n",
    "print(lm_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=pd.DataFrame(data=sc_y.inverse_transform(y_test),columns=['revolving_balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_m6 = sm.add_constant(X_test)\n",
    "X_test_cons = sm.add_constant(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction= sc_y.inverse_transform(lm_1.predict(X_test_cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.insert(loc=1,column='prediction',value=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "print('RMSE :', np.sqrt(metrics.mean_squared_error(y_test['revolving_balance'], y_test['prediction'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################RANDOM FOREST REGRESSOR#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score as rsq\n",
    "#Random Forest Regrssor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c=df3_encoded[['loan_amnt ',  'Rate_of_intrst', \n",
    "       'annual_inc',   'debt_income_ratio',\n",
    "       'delinq_2yrs', 'inq_last_6mths', 'numb_credit', 'pub_rec',\n",
    "        'total_credits', \n",
    "       'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
    "       'collection_recovery_fee', 'collections_12_mths_ex_med',\n",
    "        'acc_now_delinq',\n",
    "       'mths_since_last_delinq', 'tot_curr_bal', 'tot_colle_amt']]\n",
    "Y_c = df3_encoded[['total revol_bal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate into training and testing set\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_c  ,  # predictors\n",
    "                                                    Y_c,  # target\n",
    "    test_size=0.3,  # percentage of obs in test set\n",
    "    random_state=0)  # seed to ensure reproducibility\n",
    "\n",
    "X_train_c.shape, X_test_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''params_rand={\n",
    "#        'n_estimators':[100,200,500,1000],\n",
    "#        'max_depth':[10,15,20],\n",
    "#        'max_features':['auto', 'sqrt', 'log2'],\n",
    "#        'random_state':[42]\n",
    "#         }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''rf_model=RandomForestRegressor()\n",
    "#grid_search_rf = RandomizedSearchCV(rf_model,param_distributions=params_rand,n_jobs=-1,cv=5)\n",
    "#grid_search_rf.fit(X_train_c,y_train_c)\n",
    "#grid_search_rf.best_estimator_\n",
    "#grid_search_rf.best_params_'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=20, max_depth=15, random_state=42)\n",
    "rf_model.fit(X_train_c,y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_train=rf_model.predict(X_train_c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_r2=rsq(y_train_c,rf_pred_train)            \n",
    "rf_train_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_test=rf_model.predict(X_test_c)\n",
    "rf_test_r2=rsq(y_test_c,rf_pred_test)\n",
    "rf_test_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "print(' train RMSE :', np.sqrt(metrics.mean_squared_error(y_train_c,rf_pred_train)))\n",
    "print(' test RMSE :', np.sqrt(metrics.mean_squared_error(y_test_c,rf_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=list(rf_model.feature_importances_)\n",
    "j=list(X_train_c.columns)\n",
    "data ={'columns': j, 'feature importance':k}\n",
    "d=pd.DataFrame(data)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i m droping some of the columns whose importance is not good in our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c_1=df3_encoded[['loan_amnt ',  'Rate_of_intrst', \n",
    "       'annual_inc',   'debt_income_ratio',\n",
    "        'inq_last_6mths', 'numb_credit', 'pub_rec',\n",
    "        'total_credits', \n",
    "       'total_rec_int', \n",
    "      \n",
    "        \n",
    "       'mths_since_last_delinq', 'tot_curr_bal', ]]\n",
    "Y_c_1 = df3_encoded[['total revol_bal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate into training and testing set\n",
    "\n",
    "X_train_c_1, X_test_c_1, y_train_c_1, y_test_c_1 = train_test_split(X_c_1  ,  # predictors\n",
    "                                                    Y_c_1,  # target\n",
    "    test_size=0.3,  # percentage of obs in test set\n",
    "    random_state=0)  # seed to ensure reproducibility\n",
    "\n",
    "X_train_c_1.shape, X_test_c_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_1 = RandomForestRegressor(n_estimators=25, max_depth=20, random_state=42)\n",
    "rf_model_1.fit(X_train_c_1,y_train_c_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_train_1=rf_model_1.predict(X_train_c_1) \n",
    "rf_train_r2_1=rsq(y_train_c_1,rf_pred_train_1)            \n",
    "rf_train_r2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_test_1=rf_model_1.predict(X_test_c_1)\n",
    "rf_test_r2_1=rsq(y_test_c_1,rf_pred_test_1)\n",
    "rf_test_r2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' train RMSE :', np.sqrt(metrics.mean_squared_error(y_train_c_1,rf_pred_train_1)))\n",
    "print(' test RMSE :', np.sqrt(metrics.mean_squared_error(y_test_c_1,rf_pred_test_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1=list(rf_model_1.feature_importances_)\n",
    "j_1=list(X_train_c_1.columns)\n",
    "data_1 ={'columns': j_1, 'feature importance':k_1}\n",
    "d_1=pd.DataFrame(data_1)\n",
    "d_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### including categorical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c_2=df3_encoded[['loan_amnt ', 'terms', 'Rate_of_intrst', 'grade', 'home_ownership',\n",
    "       'annual_inc', 'verification_status', 'purpose', 'debt_income_ratio',\n",
    "       'delinq_2yrs', 'inq_last_6mths', 'numb_credit', 'pub_rec',\n",
    "        'total_credits', 'initial_list_status',\n",
    "       'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
    "       'collection_recovery_fee', 'collections_12_mths_ex_med',\n",
    "       'application_type', 'acc_now_delinq', 'Experience',\n",
    "       'mths_since_last_delinq', 'tot_curr_bal', 'tot_colle_amt' ]]\n",
    "Y_c_2 = df3_encoded[['total revol_bal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate into training and testing set\n",
    "\n",
    "X_train_c_2, X_test_c_2, y_train_c_2, y_test_c_2 = train_test_split(X_c_2  ,  # predictors\n",
    "                                                    Y_c_2,  # target\n",
    "    test_size=0.3,  # percentage of obs in test set\n",
    "    random_state=0)  # seed to ensure reproducibility\n",
    "\n",
    "X_train_c_2.shape, X_test_c_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_2 = RandomForestRegressor(n_estimators=25, max_depth=20, random_state=42)\n",
    "rf_model_2.fit(X_train_c_2,y_train_c_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_train_2=rf_model_2.predict(X_train_c_2) \n",
    "rf_train_r2_2=rsq(y_train_c_2,rf_pred_train_2)            \n",
    "rf_train_r2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_test_2=rf_model_2.predict(X_test_c_2)\n",
    "rf_test_r2_2=rsq(y_test_c_2,rf_pred_test_2)\n",
    "rf_test_r2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' train RMSE :', np.sqrt(metrics.mean_squared_error(y_train_c_2,rf_pred_train_2)))\n",
    "print(' test RMSE :', np.sqrt(metrics.mean_squared_error(y_test_c_2,rf_pred_test_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_2=list(rf_model_2.feature_importances_)\n",
    "j_2=list(X_train_c_2.columns)\n",
    "data_2 ={'columns': j_2, 'feature importance':k_2}\n",
    "d_2=pd.DataFrame(data_2)\n",
    "d_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i m dropping those columns which having importance less than 1 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c_2_a=df3_encoded[['loan_amnt ',  'Rate_of_intrst', 'grade', 'home_ownership',\n",
    "       'annual_inc',  'purpose', 'debt_income_ratio',\n",
    "        'inq_last_6mths', 'numb_credit', 'pub_rec',\n",
    "        'total_credits', \n",
    "       'total_rec_int', \n",
    "    \n",
    "         'Experience',\n",
    "       'mths_since_last_delinq', 'tot_curr_bal' ]]\n",
    "Y_c_2_a = df3_encoded[['total revol_bal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate into training and testing set\n",
    "\n",
    "X_train_c_2_a, X_test_c_2_a, y_train_c_2_a, y_test_c_2_a = train_test_split(X_c_2_a  ,  # predictors\n",
    "                                                    Y_c_2_a,  # target\n",
    "    test_size=0.3,  # percentage of obs in test set\n",
    "    random_state=0)  # seed to ensure reproducibility\n",
    "\n",
    "X_train_c_2_a.shape, X_test_c_2_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_2_a = RandomForestRegressor(n_estimators=25, max_depth=20, random_state=42)\n",
    "rf_model_2_a.fit(X_train_c_2_a,y_train_c_2_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_train_2_a=rf_model_2_a.predict(X_train_c_2_a) \n",
    "rf_train_r2_2_a=rsq(y_train_c_2_a,rf_pred_train_2_a)            \n",
    "rf_train_r2_2_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_test_2_a=rf_model_2_a.predict(X_test_c_2_a)\n",
    "rf_test_r2_2_a=rsq(y_test_c_2_a,rf_pred_test_2_a)\n",
    "rf_test_r2_2_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' train RMSE :', np.sqrt(metrics.mean_squared_error(y_train_c_2_a,rf_pred_train_2_a)))\n",
    "print(' test RMSE :', np.sqrt(metrics.mean_squared_error(y_test_c_2_a,rf_pred_test_2_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_2_a=list(rf_model_2_a.feature_importances_)\n",
    "j_2_a=list(X_train_c_2_a.columns)\n",
    "data_2_a ={'columns': j_2_a, 'feature importance':k_2_a}\n",
    "d_2_a=pd.DataFrame(data_2_a)\n",
    "d_2_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   xgboost with label encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for integer encoding using feature-engine\n",
    "from feature_engine.categorical_encoders import OrdinalCategoricalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "data_xg=pd.read_csv(r\"C:\\Users\\sak\\Desktop\\excelr_proj_1\\data_no_null.csv\",encoding='latin1')# reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns not necessary for prediction\n",
    "cols_to_drop=['Unnamed: 0','sub_grade','State','Emp_designation','last_week_pay']\n",
    "# dropping the unnecessary columns\n",
    "df_xg=data_xg.drop(columns=cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [col for col in df_xg.columns if df_xg[col].dtypes == 'O']\n",
    "\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[['A','B']] = df[['A','B']].replace([1, 3, 2], [3, 6, 7])\n",
    "df_xg[['purpose']] = df_xg[['purpose']].replace(['car','house','renewable_energy','wedding','vacation','moving','medical','educational'],\n",
    "                                              ['major_purchase','major_purchase','small_business','other','other','other','medical(or)education','medical(or)education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xg.purpose.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_enc = OrdinalCategoricalEncoder(\n",
    "    encoding_method='arbitrary',\n",
    "    variables=categorical)\n",
    "\n",
    "df_xg_label_enc = ordinal_enc.fit_transform(df_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordinal_enc.encoder_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xg_label_enc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xg_l=df_xg_label_enc[['loan_amnt ', 'terms', 'Rate_of_intrst', 'grade', 'home_ownership',\n",
    "       'annual_inc', 'verification_status', 'purpose', 'debt_income_ratio',\n",
    "       'delinq_2yrs', 'inq_last_6mths', 'numb_credit', 'pub_rec',\n",
    "       'total_credits', 'initial_list_status',\n",
    "       'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
    "       'collection_recovery_fee', 'collections_12_mths_ex_med',\n",
    "       'application_type', 'acc_now_delinq', 'Experience',\n",
    "       'mths_since_last_delinq', 'tot_curr_bal', 'tot_colle_amt']]\n",
    "Y_xg_l=df_xg_label_enc[['total revol_bal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate into training and testing set\n",
    "\n",
    "X_train_xg_l, X_test_xg_l, y_train_xg_l, y_test_xg_l = train_test_split(X_xg_l  ,  # predictors\n",
    "                                                    Y_xg_l,  # target\n",
    "    test_size=0.3,  # percentage of obs in test set\n",
    "    random_state=0)  # seed to ensure reproducibility\n",
    "\n",
    "X_train_xg_l.shape, X_test_xg_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_mod_l=xgb.XGBRegressor()\n",
    "params_l={\n",
    "        'learning_rate':[0.03,0.05,0.08,0.10,0.15,0.20,0.25,0.30],\n",
    "        'max_depth':[3,4,5,6,8,10,12,15],\n",
    "        'min_child_weight':[1,3,5,7],\n",
    "        'gamma':[0.0,0.1,0.2,0.3,0.4]\n",
    "        }\n",
    "random_search=RandomizedSearchCV(xgb_mod_l,param_distributions=params_l,n_iter=5,n_jobs=-1,cv=5,verbose=3)\n",
    "random_search.fit(X_train_xg_l,y_train_xg_l)\n",
    "random_search.best_estimator_\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_mod_l=xgb.XGBRegressor(min_child_weight=3, max_depth=6, learning_rate=0.15, gamma=0.4)\n",
    "xgb_mod_l.fit(X_train_xg_l,y_train_xg_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pred_train_l=xgb_mod_l.predict(X_train_xg_l) \n",
    "xg_train_l_r2=rsq(y_train_xg_l,xg_pred_train_l)            \n",
    "xg_train_l_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pred_test_l=xgb_mod_l.predict(X_test_xg_l)\n",
    "xg_test_l_r2=rsq(y_test_xg_l,xg_pred_test_l)\n",
    "xg_test_l_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' train RMSE :', np.sqrt(metrics.mean_squared_error(y_train_xg_l,xg_pred_train_l)))\n",
    "print(' test RMSE :', np.sqrt(metrics.mean_squared_error(y_test_xg_l,xg_pred_test_l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_xg_l=list(xgb_mod_l.feature_importances_)\n",
    "j_xg_l=list(X_train_xg_l.columns)\n",
    "data_xg_l ={'columns': j_xg_l, 'feature importance':k_xg_l}\n",
    "d_xg_l=pd.DataFrame(data_xg_l)\n",
    "d_xg_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparision=y_test_xg_l.copy()\n",
    "comparision.insert(loc=1,column='prediction',value=xg_pred_test_l)\n",
    "comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xg-boost with frequency encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_enc = CountFrequencyCategoricalEncoder(\n",
    "    encoding_method='frequency',\n",
    "    variables=categorical)\n",
    "\n",
    "df_xg_fre_enc = ordinal_enc.fit_transform(df_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xg_f=df_xg_fre_enc[['loan_amnt ', 'terms', 'Rate_of_intrst', 'grade', 'home_ownership',\n",
    "       'annual_inc', 'verification_status', 'purpose', 'debt_income_ratio',\n",
    "       'delinq_2yrs', 'inq_last_6mths', 'numb_credit', 'pub_rec',\n",
    "       'total_credits', 'initial_list_status',\n",
    "       'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
    "       'collection_recovery_fee', 'collections_12_mths_ex_med',\n",
    "       'application_type', 'acc_now_delinq', 'Experience',\n",
    "       'mths_since_last_delinq', 'tot_curr_bal', 'tot_colle_amt']]\n",
    "Y_xg_f=df_xg_fre_enc[['total revol_bal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate into training and testing set\n",
    "\n",
    "X_train_xg_f, X_test_xg_f, y_train_xg_f, y_test_xg_f = train_test_split(X_xg_f  ,  # predictors\n",
    "                                                    Y_xg_f,  # target\n",
    "    test_size=0.3,  # percentage of obs in test set\n",
    "    random_state=0)  # seed to ensure reproducibility\n",
    "\n",
    "X_train_xg_f.shape, X_test_xg_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_mod_f=xgb.XGBRegressor()\n",
    "params_f={\n",
    "        'learning_rate':[0.03,0.05,0.08,0.10,0.15,0.20,0.25,0.30],\n",
    "        'max_depth':[3,4,5,6,8,10,12,15],\n",
    "        'min_child_weight':[1,3,5,7],\n",
    "        'gamma':[0.0,0.1,0.2,0.3,0.4]\n",
    "        }\n",
    "random_search_f=RandomizedSearchCV(xgb_mod_f,param_distributions=params_f,n_iter=5,n_jobs=-1,cv=5,verbose=3)\n",
    "random_search_f.fit(X_train_xg_f,y_train_xg_f)\n",
    "random_search_f.best_estimator_\n",
    "random_search_f.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_mod_f=xgb.XGBRegressor(min_child_weight=5, max_depth=3, learning_rate=0.08, gamma=0.1)\n",
    "xgb_mod_f.fit(X_train_xg_f,y_train_xg_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pred_train_f=xgb_mod_f.predict(X_train_xg_f) \n",
    "xg_train_f_r2=rsq(y_train_xg_f,xg_pred_train_f)            \n",
    "xg_train_f_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pred_test_f=xgb_mod_f.predict(X_test_xg_f)\n",
    "xg_test_f_r2=rsq(y_test_xg_f,xg_pred_test_f)\n",
    "xg_test_f_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' train RMSE :', np.sqrt(metrics.mean_squared_error(y_train_xg_f,xg_pred_train_f)))\n",
    "print(' test RMSE :', np.sqrt(metrics.mean_squared_error(y_test_xg_f,xg_pred_test_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_xg_f=list(xgb_mod_f.feature_importances_)\n",
    "j_xg_f=list(X_train_xg_f.columns)\n",
    "data_xg_f ={'columns': j_xg_f, 'feature importance':k_xg_f}\n",
    "d_xg_f=pd.DataFrame(data_xg_f)\n",
    "d_xg_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparision_f=y_test_xg_f.copy()\n",
    "comparision_f.insert(loc=1,column='prediction',value=xg_pred_test_f)\n",
    "comparision_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Model Build and Anlaysis (**Complete representation in PPT)\n",
    "  - Linear Regression: \n",
    "        With a train/test split of 70/30 ,R-squared and RMSE values are 0.261( both NULL and NOT NULL) and 18947.63(NULL),18851.73(NOT NULL) respectively.\n",
    "        #The RMSE values are pretty close and could be cosidered a good fit but not until other models are proven otherwise.\n",
    "  - Random Forest: \n",
    "        RF model did not prove to be a good model considering the variation in the R-sqaured & RMSE values on 4 different types of models.\n",
    "        The best model amongst the 4 was NOT NULL datatype, ONLY NUMERICAL variables, R-squared values being 0.66(TRAIN) & 0.33(TEST) and RMSE values being 13296(TRAIN) & 17496(TEST).\n",
    "  - XGBoost :\n",
    "        The best model amongst all types as the RMSE values were pretty close and the R-squared values were fairly reasonable considering the type of data provided.\n",
    "    \n",
    "    \n",
    "\n",
    "    Finally, a model is chosen to proceed with the final build and eventually deploy a web application predicting the revolving balance.    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
